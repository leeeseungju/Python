# -*- coding: utf-8 -*-
"""LLM_HanaCard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pB_M6bIOYCugWjK053PkuxMw6gp_t6fO

# 1. 1. 데이터셋 구성

## 1.1 set model and dataset
"""

### dataset 폴더 내 TL_multiple_choice.json

# 허깅페이스: AI 개발자들이 자신들의 모델이나 학습데이터등을 관리하는 생태계 (사이트)
# 데이터셋 라이브러리는 허깅페이스에서 요구하는 데이터포맷 으로 변환 및 관련 데이터 처리 함수 내장됨
!pip install datasets==2.21.0

# 라이브러리 설치
import requests
import json
import pandas as pd
import os
from datasets import Dataset
import huggingface_hub

# 구글 마운트
from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM

# json.loads("./dataset/TL_multiple_choice.json")

# JSON 파일 경로
folder_path = '/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/Dataset_HanaCard_qna'
final_data = []

folder = os.listdir(folder_path)
len(folder)

# for문을 사용하여 폴더 내에 있는 json 파일 꺼내기
# 폴더 내 모든 파일 이름 반복
for file_name in os.listdir(folder_path):
    if file_name.endswith('.json'):
        file_path = os.path.join(folder_path, file_name)
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)

        for entry in raw_data:
            instructions = entry.get('instructions', [])
            for block in instructions:
                for item in block.get('data', []):
                    q = item.get('instruction', '').strip()
                    context = item.get('input', '').strip()
                    answer = item.get('output', '').strip()

                    if q and context and answer:
                        final_data.append({
                            "instruction": "아래 상담 내용을 참고하여 질문에 정확하고 간결하게 답하시오.",
                            "input": f"질문: {q}\n\n상담 내용:\n{context}",
                            "output": answer
                        })

output_path = '/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/alpaca_format_data.jsonl'
with open(output_path, 'w', encoding='utf-8') as f:
    for entry in final_data:
        json.dump(entry, f, ensure_ascii=False)
        f.write('\n')

print(f"\n변환 완료, 저장 위치: {output_path}")

answerDf = pd.DataFrame(final_data)

pd.DataFrame(answerDf)

# # Dataset 형태로 변환
dataset = Dataset.from_pandas(answerDf)

# # 허깅페이스 로그인 ( https://huggingface.co/ 회원가입 후 DATASET 하나 추가 이후 API 키 생성)
huggingface_hub.login(os.getenv("HF_TOKEN"))

"""## 1.3 Push to Hugging Face"""

# # 허깅페이스에 업로드
dataset.push_to_hub("leeseungju/hana-card-qa")

"""# 2. 파인튜닝

## 2-1. Install Library
"""

try:
    import google.colab
    inColab = True
except ImportError:
    inColab = False

# Hugging Face에서 개발한 도구로, 딥러닝 모델의 훈련과 추론 속도를 높이기 위한 최적화
# PEFT( Parameter-Efficient Fine-Tuning) 딥러닝 모델의 파라미터를 효율적으로 미세 조정하는 방법 제공
# bitsandbytes 딥러닝 모델을 더 작은 메모리 풋프린트로 실행할 수 있게 해주는 라이브러리
# transformers 사전 학습된 모델들을 제공하는 도구, 대형 모델을 쉽게 사용할 수 있게 해줌
# trl(Transformers Reinforcement Learning) 사전 학습된 트랜스포머 모델에 강화 학습 적용하여 특정 작업에 맞게 모델을 더 정밀하게 조정하는 데 사용
# dataset 다양한 데이터셋을 쉽게 로드, 처리, 변환, 분석할 수 있는 라이브러리
# -U 옵션: 지정된 패키지가 이미 설치되어 있는 경우에도 최신 버전으로 업그레이드
if inColab == True:
    !pip install -U pandas==2.2.2 numpy==2.0.2 scipy==1.14.1 accelerate==1.6.0 peft==0.15.2 bitsandbytes==0.45.5 transformers==4.51.3 trl==0.16.1 datasets==3.5.0 tensorboard==2.19.0

# 라이브러리 설치
!pip install trl
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import load_dataset
import torch
import huggingface_hub
import os
from datetime import datetime

if inColab == True:
    from google.colab import drive
    drive.mount("/content/gdrive")

"""## 2-2. set model and dataset"""

# 허깅페이스 로그인
huggingface_hub.login(os.getenv("HF_TOKEN"))

# 모델, 데이터셋 로드
base_model = "meta-llama/Meta-Llama-3-8B-Instruct"
dataset_name = "leeseungju/hana-card-qa"
dataset = load_dataset("leeseungju/hana-card-qa", split="train")

# 데이터를 파인튜닝 할 수 있도록 알파카 포멧(instruction, input, output) 형태로 변환
def convert_to_alpaca_format(example):
    instruction = example["instruction"]
    input_text = example.get("input", "")
    output = example["output"]

    if input_text:
        prompt = f"### 질문:\n{instruction}\n\n### 입력:\n{input_text}\n\n### 답변:\n{output}"
    else:
        prompt = f"### 질문:\n{instruction}\n\n### 답변:\n{output}"

    example["text"] = prompt
    return example

"""## 2-3. Config efficient fine-tuning with low-rank adaptation."""

from transformers import BitsAndBytesConfig
import torch

torch.cuda.get_device_capability()[0]

# 현재 사용 중인 GPU의 주요 아키텍처 버전을 반환 8버전 이상 시 bfloat16 활용
# NVIDIA Ampere 아키텍처 이상 시에만 처리
# 정확도를 위하여 float 16이 타도록 강제설정 8->10 ★ 향후 변경 필요
if torch.cuda.get_device_capability()[0] >= 10:
    # 고속 attention 메커니즘을 구현하는 라이브러리
    !pip install -qqq flash-attn
    attn_implementation = "flash_attention_2"
    torch_dtype = torch.bfloat16
else:
    attn_implementation = "eager"
    torch_dtype = torch.float16


# BitsAndBytesConfig 객체활용 양자화 설정
quant_config = BitsAndBytesConfig(
    # 모델을 4비트 양자화하여 로드할지 여부 결정
    load_in_4bit=True,
    # 양자화 방법 (nf4: Non-Uniform Quantization, "nf4","fp16 등))
    bnb_4bit_quant_type="nf4",
    # (4비트 양자화 시 사용할 데이터 타입, "torch.float16, bfloat16, float32 등)
    bnb_4bit_compute_dtype=torch_dtype,
    # 이중 양자화 사용여부 (이중 양자화는 양자화 과정에서 정밀도 높이기 위해 활용, 대신 더 연산은 복잡)
    bnb_4bit_use_double_quant=False,
)

"""## 2-4. Load Pre-trained Language Model"""

model = AutoModelForCausalLM.from_pretrained(
    # 불러올 모델 정의
    base_model,
    # 모델 양자화 설정값
    quantization_config=quant_config,
    # 모델의 레이어를 할당할 장치 ("":0 -> 전체 모델을 GPU 0에 할당, "auto"는 알아서, "{"layer_0":0, ... 형태로 레이어별 할당 가능)
    # device_map={"": 0}
    device_map="auto"
)
# 캐시 사용 여부 (모델 출력 매번 새로 계산)
# 새로운 데이터에 대한 계산 시 저장하는게 불필요함 또한 True 시 캐시 저장할 메모리 사용하여 메모리 사용량 증가함
model.config.use_cache = False
# 모델읜 pretraining tensor parallelism 설정 1인경우 병렬처리 안하고 단일장치 활용 2이상 시 병렬 처리를 분산 가능
# 병렬 처리 시 추가적인 메모리 오버헤드 발생 가능함
model.config.pretraining_tp = 1

"""## 2-5. Load Pre-trained Language Model Tokenizer"""

from datasets import load_dataset

datasetCommon = load_dataset("leeseungju/hana-card-qa", split="train")

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
# Must add EOS_TOKEN at response last line
tokenizer.pad_token = tokenizer.eos_token

# 알파카 포맷으로 text 필드 생성
def convert_to_alpaca_format(example):
    instruction = example["instruction"]
    input_text = example.get("input", "")
    output = example["output"]

    if input_text:
        text = f"### 질문:\n{instruction}\n\n### 추가 정보:\n{input_text}\n\n### 답변:\n{output}"
    else:
        text = f"### 질문:\n{instruction}\n\n### 답변:\n{output}"

    example["text"] = text
    return example

datasetCommon = datasetCommon.map(convert_to_alpaca_format)

# ★수정 포인트!!! 기존 # tokenizer.padding_side = "right"
EOS_TOKEN = tokenizer.eos_token
def prompt_eos(sample):
    sample['text'] = sample['text']+EOS_TOKEN
    return sample
datasetCommon = datasetCommon.map(prompt_eos)

"""## 2-6. Config training parameter for LoRA (Parameter-Efficient Fine-Tuning (PEFT)

https://huggingface.co/docs/peft/conceptual_guides/lora
"""

# LoRA 적용
peft_params = LoraConfig(
    # lora 방식의 튜닝에서 저차원 병렬 레이어 학습 ( 값이 크면 저차원 병렬 레이어 영향이 커지고 작으면 줄어듬, 기본 16, 1~128)
    lora_alpha=32,
    # 과적합 방지 (0.1에서 증가 시킴, 0.0~0.3)
    lora_dropout=0.1,
    # LoRA 저차원 공간의 차원 수 (값이 커지면 파라미터 공간 커지지만 계산비용 증가 4, 8, 16, 32, 64 로 튜닝, 4~128)
    r=8,
    # LoRA 모델의 바이어스 파라미터 적용여부 None = 미적용 ("none" or "all", "lora_only" 설정 가능)
    bias="none",
    # LoLA 모델의 작업 유형 (GPT와 같은 인과모델) ("CASUAL_LM": GPT, "SEQ_CLASSSIFICATION": 시퀀스 분류, "TOKEN_CLASSIFICATION": 토큰 분류, "SEQ_2_SEQ_LM": 번역모델)
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # SOLAR 최적화용
)

from peft import get_peft_model

model = get_peft_model(model, peft_params)

import torch
torch.backends.cudnn.benchmark = True

training_params = TrainingArguments(
    # 학습 결과와 체크포인트를 저장할 디렉토리 경로
    output_dir="./results",
    # 전체 데이터셋을 반복학습 주기 (1~50 정도 활용)
    num_train_epochs=50,
    # 각 GPU 장치에서 사용하는 배치 크기 (배치 크기에 따라 가중치 업데이트 1~64)
    # 배치크기는 모델히 한번에 처리하는 데이터 샘플의 수
    per_device_train_batch_size=1,
    #그래디언트 누적을 통한 배치 크기 증가 (여러 작은 배치를 합쳐서 효과적으로 큰 배치 크기를 적용 가능) *4 적용 시 그래디언트 누적 4개 후 한번에 가중치 업데이트
    # 배치크기가 4배 증가한 효과를 가지지만 실제 메모리 사용은 배치 1개 크기만 사용
    gradient_accumulation_steps=4,
    # 사용할 옵티마이저 설정 ("adamw, paged_adamw_32bit","adamw_torch")
    optim="adamw_torch",
    # 학습 중 모델 체크포인트 저장 주기 (스템 수)
    save_steps=50,
    # 학습 중 로그를 기록할 주기(스텝 수)
    logging_steps=50,
    save_total_limit=1,
    # 학습률 설정 (모델의 파라미터 업데이트 시 스템 크기, 1e-6 ~ 1e-2))
    learning_rate=2e-4,
    # 모델의 가중치에 패널티 적용하여 과적합 방지 (0.0은 정규화 안함, 0.0 ~ 0.1)
    weight_decay=0.001,
    # 16비트 부동 소수점(FP16) 연산을 사용하여 메모리 사용량과 계산속도 개선 (사용시 정밀도 감소, 32비트 부동소수점보다 절반)
    fp16=False,
    # bfloat16 연산을 사용하여 학습 FP16대비 더 넓은 지수 범위 (사용시 정밀도 감소, 32비트 부동소수점보다 절반)
    bf16=False,
    # (그래디언트 클리핑을 위한 최대 노름 값 설정, 0.1 ~ 10)
    # 그래디언트 클리핑은 그래디언트의 크기가 너무 커서 학습이 불안정해지는 것을 방지하기 위한 기법 0.3 초과하지 않도록 함 (크면 가중치가 너무 크게업데이트됨)
    max_grad_norm=0.3,
    # 전체 학습단계 수 (-1 인 경우 epochs, 이외에는 스탭수,  -1~100000)
    max_steps=-1,
    # 학습률 warmup비율 설정 (학습 초기에 학습률을 서서히 증가시켜 안정적인 학습, 0.0~0.5)
    # 너무 높으면 학습률이 너무 늦어짐
    warmup_ratio=0.03,
    # 배치 내 시쿼스 길이를 그룹화하여 패딩을 최소화 (데이터 시권스길이 유사한것 끼리 그룹화하여 메모리 효율설 높임)
    group_by_length=True,
    # 학습률 스케줄러의 유형 설정 ("linear","cosine","constant","polynomial" 등) * constant는 학습률을 일정하게 유지
    lr_scheduler_type="constant",
    # 학습 로그를 보고할 플랫폼 설정 (아웃풋 디렉토리 참고) -> tensorboard --logdir=./results/runs * wandb로 설정 가능
    report_to="tensorboard",
    dataloader_num_workers=8  # 데이터 로딩 속도 최적화
)

"""## 2-7. Train Model"""

# from datasets import Dataset
# text_list = datasetCommon["text"]  # 또는 datasetCommon.map(lambda x: x["text"])

# # Dataset 객체로 변환 (필요시)
# train_dataset = Dataset.from_dict({"text": text_list})
# train_dataset

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()

model = get_peft_model(model, peft_config)

# gradient checkpoint 켜기
model.gradient_checkpointing_enable()

model = get_peft_model(model, peft_config)
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()

if hasattr(model.config, "use_flash_attention_2"):
    model.config.use_flash_attention_2 = True

torch.cuda.empty_cache()

from peft import prepare_model_for_kbit_training

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)

# 학습 설정
trainer = SFTTrainer(
    # 학습할 모델
    model=model,
    # 모델 학습에 사용할 데이터셋
    train_dataset=datasetCommon,
    #  PEFT(파라미터 효율적 미세 조정) 설정 정의
    # peft_config=peft_params,
    # 데이터셋에서 학습 데이터셋 텍스트 필드 이름
    # dataset_text_field="text",
    # 입력 시퀀스 최대 길이 (128 ~ 1024) * 길이가 길수록 더많은 컨텍스트를 모델에 제공가능 단, 메모리 사용량 증가
    # max_seq_length=None,
    # 모델과 함께 사용할 토크나이저
    # tokenizer=tokenizer,
    args=training_params,
    # 입력 시퀀스 패킹여부 (패킹 시 짧고/긴 시퀀스를 혼합하여 배치 처리 서능 개선)
    # packing=False,
)

# 학습 시작
trainer.train()

# 모델 저장
# now_str = datetime.now().strftime("%Y_%m_%d_%H")
save_path = f"/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/llama_tune_2025_05_02_05"
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)

from peft import get_peft_model

peft_model = get_peft_model(model, peft_config)

# 어댑터 저장 경로 설정
adapter_path = "/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/adapters/llama_adapter_2025_05_02"

# LoRA adapter 저장
peft_model.save_pretrained(adapter_path)

"""## 2-8. Verify"""

from transformers import pipeline

# text-generation 작업을 위한 파이프라인 객체 생성
pipe = pipeline(
    # 작업 유형: 텍스트 생성
    task="text-generation",
    # 사용할 모델
    model=model,
    # 모델의 토크나이저
    tokenizer=tokenizer,
    # 모델에 전달할 추가 인자: float16 데이터 타입을 사용하여 모델을 로드 ("bfloat16"으로 학습했다면 bfloat16 사용)
    model_kwargs={"torch_dtype": torch.float16},
    # 입력 텍스트가 너무 길 경우 잘라내기
    truncation=True
)

def extract_response_llama3(question):
    # 사용자 질문을 포함하는 메시지 리스트를 생성
    messages = [
        {"role": "system", "content": ""},       # 시스템 메시지: 일반적으로 시스템의 지침이나 상태값 저장
        {"role": "user", "content": question},   # 사용자 메시지: 사용자 질문을 포함
    ]
    # 메시지를 토크나이저를 사용하여 모델의 입력 형식에 맞게 변환
    prompt = pipe.tokenizer.apply_chat_template(
        messages,                             # 메시지 리스트
        tokenize=False,                       # 토크나이즈를 하지 않음
        add_generation_prompt=True            # 텍스트 생성을 위한 프롬프트 추가
    )
    # 텍스트 생성 종료 토큰 ID 목록
    terminators = [
        pipe.tokenizer.eos_token_id,                        # End Of Sequence 토큰 ID
        pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")  # 사용자 정의 종료 토큰 ID
    ]
    outputs = pipe(
        prompt,                          # 생성할 프롬프트
        max_new_tokens=256,              # 생성할 최대 토큰 수
        eos_token_id=terminators,        # 텍스트 생성 종료를 위한 토큰 ID
        do_sample=False,                  # 샘플링을 사용하여 다음 토큰(텍스트) 생성 (True 시 토큰 무작위 선택 하여 창의성  높임, False 시 창의성 낮춤)
        temperature=0.1,                 # 샘플링의 온도 설정: 낮은 온도는 자유도를 주지 않음 (0보다 크고1보다 작아야함)
        # top_p=0.9,                       # 상위 90%의 확률을 가진 토큰중에서만 무작위로 선택 do_sample true인 경우에만 설정
        num_return_sequences=1           # 생성할 시퀀스의 수: 한 개만 생성
    )
    generated_text = outputs[0]['generated_text']        # 생성된 텍스트를 추출
    response_lines = generated_text.strip().split('\n')  # 텍스트를 줄 단위로 분리
    meaningful_response = response_lines[-1]             # 마지막 줄을 응답으로 선택
    return meaningful_response

question = "고객의 한도 심사 결과는 언제 나와? "
prompt = f"### 질문:\n{question}\n\n### 답변:"
response = extract_response_llama3(prompt)
print(response)

question = "카드 결제일 변경 후 재변경이 어려운 기간은 언제까지인가?"
prompt = f"### 질문:\n{question}\n\n### 답변:"
response = extract_response_llama3(prompt)
print(response)

question = "카드 사용 한도에 포함되는 항목은 뭐가 있어?"
prompt = f"### 질문:\n{question}\n\n### 답변:"
response = extract_response_llama3(prompt)
print(response)

question = "카드 한도 신청 즉시 해당 금액을 사용할 수 있나요?"
prompt = f"### 질문:\n{question}\n\n### 답변:"
response = extract_response_llama3(prompt)
print(response)

savePath = f"/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/llama_tune_2025_05_02_05"

if inColab == True:
    savePath = f"/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/llama_tune_2025_05_02_05"
    trainer.save_model(savePath)
else:
    savePath = "./models/llama_tune_common"
    trainer.save_model(savePath)

"""# 3. 모델 생성"""

try:
    import google.colab
    inColab = True
except ImportError:
    inColab = False

if inColab == True:
    !pip install -U pandas==2.2.2 numpy==2.0.2 scipy==1.14.1 accelerate==1.6.0 peft==0.15.2 bitsandbytes==0.45.5 transformers==4.51.3 trl==0.16.1 datasets==3.5.0 tensorboard==2.19.0

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging
)
from transformers import AutoConfig,AutoModel
import torch
from peft import PeftModel, PeftConfig

import huggingface_hub
huggingface_hub.login(os.getenv("HF_TOKEN"))

"""## 3-1. Load base and new model"""

# from transformers import AutoConfig, AutoModel, AutoTokenizer
# config = AutoConfig.from_pretrained("your model name", revision=revision)
# model = AutoModel.from_pretrained("your model name", revision=revision)
# tokenizer = AutoTokenizer.from_pretrained("your model name", revision=revision)

## base 모델
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto"
)
## 파인튜닝한 모델 저장 위치 설정
new_model = "/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/models/llama_tune_2025_05_02_05"

### 베이스모델 불러오기
# 모델 로드 (device_map=None으로 전체 로딩)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float16,
    device_map=None,
    low_cpu_mem_usage=False  # 가능하면 False로 하여 확실히 로드
)

### 토크나이저 불러오기
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

save_path = "/content/gdrive/MyDrive/Colab Notebooks/fintech_edu_2025/중간고사/LLM/models/llama_tune_2025_05_02_05"

# LoRA 적용
model = get_peft_model(model, peft_config)

# 모든 레이어를 실제 메모리로 이동
model = model.to("cuda" if torch.cuda.is_available() else "cpu")

# 저장
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

# PEFT 설정 로드
peft_model = get_peft_model(base_model, peft_config)
print(peft_config)

# 베이스모델에 어댑터 적용
model = PeftModel.from_pretrained(model, new_model)

"""### 어뎁터 연결 모델 정상작동 확인"""

def convert_to_alpaca_format(example):
    instruction = example["instruction"]
    input_text = example.get("input", "")
    output = example["output"]

    if input_text:
        text = f"### 질문:\n{instruction}\n\n### 추가 정보:\n{input_text}\n\n### 답변:\n{output}"
    else:
        text = f"### 질문:\n{instruction}\n\n### 답변:\n{output}"

    example["text"] = text
    return example

datasetCommon = datasetCommon.map(convert_to_alpaca_format)

def extract_response_llama3(question):
    messages = [
        {"role": "system", "content": "당신은 무조건 한국어로, 짧고 정확하게 답변하는 상담원입니다. 가능한 짧고 단정적인 답변을 해주세요."},
        {"role": "user", "content": question},
    ]

    prompt = pipe.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    terminators = [
        pipe.tokenizer.eos_token_id,
        pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    outputs = pipe(
        prompt,
        max_new_tokens=50,
        eos_token_id=terminators,
        do_sample=False,   # do_sample 끄기
        temperature=0.0,   # 확정적으로
        top_p=1.0,
        num_return_sequences=1
    )

    generated_text = outputs[0]['generated_text']
    response_lines = generated_text.strip().split('\n')
    meaningful_response = response_lines[-1]

    return meaningful_response
question = "카드 결제 취소 후 환급 처리가 완료되는 기간은 며칠 정도 소요되나요?"
response = extract_response_llama3(question)
print(response)

model = model.to("cpu")  # 모델을 CPU로 이동
merged_model = model.merge_and_unload()

from huggingface_hub import login
huggingface_hub.login(os.getenv("HF_TOKEN"))

# set your HF repository
hfAddr = "leeseungju/hana-card-qa-llama3.8b"

# save model and tokenizer to HF hub
merged_model.push_to_hub(hfAddr)
tokenizer.push_to_hub(hfAddr)

# 질문
question = "카드 결제 취소 후 환급 처리가 완료되는 기간은 며칠 정도 소요되나요?"

# 답변 받기
response = extract_response_llama3(question)
print(response)

# 질문
question = "카드 결제일 변경 후 재변경이 어려운 기간은 언제까지인가?"

# 답변 받기
response = extract_response_llama3(question)
print(response)

"""★ 참고 코드"""

### 리더보드 올리기전 테스트 코드 (모델 이상여부 확인, 에러 시 등록 X)

from transformers import AutoConfig, AutoModel, AutoTokenizer
hfAddr = "leeseungju/hana-card-qa-llama3.8b"
config = AutoConfig.from_pretrained(hfAddr, revision="main")
model = AutoModel.from_pretrained(hfAddr, revision="main")
tokenizer = AutoTokenizer.from_pretrained(hfAddr, revision="main")

"""## 4. 테스트"""

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# 1. Hugging Face repo ID
repo_id = "leeseungju/hana-card-qa-llama3.8b"

# 2. tokenizer & model 불러오기
tokenizer = AutoTokenizer.from_pretrained(repo_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    repo_id,
    device_map="auto",  # GPU 자동 할당
    torch_dtype="auto"
)

# 3. 파이프라인 구성
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto")

# 4. ChatML 형식 프롬프트
question = "대출 신청을 하기 위해 신용 조회 동의가 필요한가요?"
prompt = "<|begin_of_text|><|user|>\n대출 신청을 하기 위해 신용 조회 동의가 필요한가요? 답변은 한국어로 해줘\n<|assistant|>"

# 5. 모델에 질문 던지기
response = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)

# 6. 출력 보기
print(response[0]["generated_text"])

question = "카드사에서 직접 휴대폰 요금 자동이체를 변경할 수 있나요?"
prompt = f"""<|begin_of_text|><|user|>
{question}

(주의: 아래 질문에 대해 반드시 **한국어**로 답변해주세요. 영어로 대답하지 마세요. 이모지도 사용하지 마세요. 친절하고 명확하게 설명해주세요.)

<|assistant|>"""

response = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)
print(response[0]["generated_text"].split("<|assistant|>")[-1].strip())