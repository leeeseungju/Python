# -*- coding: utf-8 -*-
"""0407_공공데이터 수집_이승주.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xFdB-phsVE4FVOCWH9OtOdgqfpkiradX

## 1. 국민연금공단_국민연금 가입 사업장 내역 (XML)
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import math  # 페이지 수 계산용

# 대상 URL과 인증 키
base_url = "http://apis.data.go.kr/B552015/NpsBplcInfoInqireService/getBassInfoSearch"
serviceKey = "j8/1etl4nCh8vuNTeM6XGFbKSinTMNhjH824+coUqsXHO/PAMi58WdARBoUpdZFvIWv/qOvS+YJEwlfDXhF6OA=="

# 파라미터 지정
base_params = {
    'serviceKey': serviceKey,
    'numOfRows': 100, # 한 페이지에 출력할 자료 수
    'pageNo': 1,
    'dataType': 'XML',
    'ldong_addr_mgpl_dg_cd': 11,   # 법정동주소광역시도코드 (서울특별시)
    'ldong_addr_mgpl_sggu_cd': 560, # 법정동주소시군구코드 (영등포구)
    'ldong_addr_mgpl_sggu_emd_cd': 102 # 법정동주소읍면동코드 (영등포동1가)
}

response = requests.get(base_url, params=base_params)
soup = BeautifulSoup(response.content, 'xml') # XML 파싱

# 페이지를 넘기며 반복해서 자료를 찾기 위해 전체 페이지 수 먼저 계산
total_count = int(soup.find('totalCount').text)
num_of_rows = int(base_params['numOfRows'])
total_pages = math.ceil(total_count / num_of_rows)

print(f"총 데이터 수: {total_count}개, 총 페이지 수: {total_pages}페이지")

# 전체 데이터 저장 리스트
all_data = []

# 페이지 수 만큼 반복
for page in range(1, total_pages + 1):
    print(f"현재 페이지: {page}/{total_pages}")
    params = base_params.copy()
    params['pageNo'] = page

    response = requests.get(base_url, params=params)
    soup = BeautifulSoup(response.content, 'xml')
    items = soup.find_all('item') # 공공데이터이므로 item으로 검색

    for item in items:
        item_dict = {
            '사업장등록번호': item.find('bzowrRgstNo').text if item.find('bzowrRgstNo') else None,
            # '데이터생성연월': item.find('dataCrtYm').text if item.find('dataCrtYm') else None,
            # '시도코드': item.find('ldongAddrMgplDgCd').text if item.find('ldongAddrMgplDgCd') else None,
            # '시군구코드': item.find('ldongAddrMgplSgguCd').text if item.find('ldongAddrMgplSgguCd') else None,
            # '읍면동코드': item.find('ldongAddrMgplSgguEmdCd').text if item.find('ldongAddrMgplSgguEmdCd') else None,
            '사업장명': item.find('wkplNm').text if item.find('wkplNm') else None,
            '도로명주소': item.find('wkplRoadNmDtlAddr').text if item.find('wkplRoadNmDtlAddr') else None,
            # '업체구분코드': item.find('wkplStylDvcd').text if item.find('wkplStylDvcd') else None,
            # '취득상태코드': item.find('wkplJnngStcd').text if item.find('wkplJnngStcd') else None,
            '순번': item.find('seq').text if item.find('seq') else None,
        }
        all_data.append(item_dict)

# DataFrame 변환
df = pd.DataFrame(all_data)

# 결과 출력
print(df.to_string())

"""## 2. 국립중앙의료원_전국 약국 정보 조회 서비스 (XML)"""

import pandas as pd
import requests, bs4
from bs4 import BeautifulSoup

# 대상 URL에 인증키를 직접 넣음
targetUrl = "http://apis.data.go.kr/B552657/ErmctInsttInfoInqireService/getParmacyListInfoInqire?ServiceKey=j8%2F1etl4nCh8vuNTeM6XGFbKSinTMNhjH824%2BcoUqsXHO%2FPAMi58WdARBoUpdZFvIWv%2FqOvS%2BYJEwlfDXhF6OA%3D%3D&Q0=서울특별시&Q1=종로구&ORD=ADDR&pageNo=1&numOfRows=100"

resp = requests.get(targetUrl)
xml_content = resp.text

soup = BeautifulSoup(xml_content, "xml") # XML 파싱

items = soup.find_all('item') # 공공데이터이므로 item으로 검색

items

data = []
try:
  for item in items: # 원하는 컬럼 값만 가져오기
      dutyName = item.find('dutyName').text if item.find('dutyName') else None
      dutyAddr = item.find('dutyAddr').text if item.find('dutyAddr') else None
      dutyTel1 = item.find('dutyTel1').text if item.find('dutyTel1') else None

      data.append({
          '약국명': dutyName,
          '주소': dutyAddr,
          '전화번호': dutyTel1
      })
except Exception as e: # 예외처리
    print(e)

# DataFrame 변환
df = pd.DataFrame(data)

# 결과 출력
print(df)

"""## 3. 자동차등록대수현황 시도별 (JSON)"""

import requests
import pandas as pd

targetUrl2 = "https://kosis.kr/openapi/Param/statisticsParameterData.do?method=getList&apiKey=Y2U5OGEyMjYzMzY2OTgzODYxMThlNWM4YWY0NTUwNDg=&itmId=13103873443T1+13103873443T2+13103873443T3+13103873443T4+&objL1=13102873443A.0001+&objL2=ALL&objL3=ALL&objL4=&objL5=&objL6=&objL7=&objL8=&format=json&jsonVD=Y&prdSe=M&newEstPrdCnt=3&orgId=116&tblId=DT_MLTM_5498"

df = pd.read_json(targetUrl2)

try:
    jsonData = pd.read_json(targetUrl2)
except Exception as e:
    print(e)

print(df.to_string())

# 전국 데이터는 너무 많아 서울특별시 내 3개월 항목만 옵션 선택하여 가져옴
# 3개월간 월별 서울 강서구의 영업용 차량 중 화물, 특수 차종 등록 데이터를 조회하고자 함

# 데이터를 담을 리스트
carData = []

# 날짜, 시군구명, 차량 유형, 차종 구분 리스트
dates = [202412, 202501, 202502] # 서치 원하는 날짜 리스트
cities = ['강서구'] # 시군구 구분
carTypes = ['영업용']  # 차량 유형 구분
carType2 = ['화물', '특수'] # 차종 구분

try:
    for date in dates: # 날짜 하나씩 꺼내기 (연도주차형식)
        for city in cities: # 구 하나씩 꺼내기
            for carType in carTypes: # 차량 유형 하나씩 꺼내기
                for idx in range(len(df)):  # 전체 데이터프레임 인덱스 순서대로 접근
                    current_date = int(df.loc[idx, 'PRD_DE'])
                    current_city = df.loc[idx, 'C2_NM']
                    current_type = df.loc[idx, 'ITM_NM']
                    current_type2 = df.loc[idx, 'C3_NM']

                    # 날짜 AND 시군구 AND 차량 유형 AND (차종_화물 OR 차종_특수) 를 만족할 경우 append
                    if (current_date == date) and (current_city == city) and (current_type == carType) and (('화물' in current_type2) or ('특수' in current_type2)):
                        carData.append({
                            '날짜': df.loc[idx, 'PRD_DE'],
                            '광역시/도': df.loc[idx, 'C1_NM'],
                            '시군구명': df.loc[idx, 'C2_NM'],
                            '구분': df.loc[idx, 'ITM_NM'],
                            '차종': df.loc[idx, 'C3_NM'],
                            '차량 수': df.loc[idx, 'DT']
                        })
except Exception as e: # 예외처리
    print(e)

# DataFrame으로 변환
filtered_df = pd.DataFrame(carData)

# 결과 출력
print(filtered_df)